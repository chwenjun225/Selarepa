import os 
import cv2
import fire
import shutil
import numpy as np 
from PIL import Image 
from pathlib import Path 
from typing import List 
from typing import Tuple  
from datetime import datetime

import torch 
from ultralytics import YOLO

from transformers import AutoModel 
from transformers import AutoTokenizer 
from transformers import AutoProcessor 

# -------------------------------------- Inference with cam360 datasets --------------------------------------

def collect_video_paths(root_dir: str) -> List[Path]:
    """
    Recursively collect all .mp4 video file paths from a root directory.

    Args:
        root_dir (str): Root directory to search for video files.

    Returns:
        List[Path]: List of paths to video files.
    """
    root_path = Path(root_dir)
    return list(root_path.rglob("*.mp4"))


def write_yolo_txt_with_labels(
        result,
        txt_filename: Path,
        class_labels: dict[int, str],
        use_label_str: bool = False, # False --> cho label báº±ng ID, True --> cho label báº±ng chá»¯ 
        conf_threshold: float = 0.4
    ) -> None:
    """
    Ghi káº¿t quáº£ phÃ¡t hiá»‡n tá»« YOLO vÃ o file .txt.

    Args:
        result: Äá»‘i tÆ°á»£ng káº¿t quáº£ tá»« YOLO (results[0]).
        txt_filename (Path): ÄÆ°á»ng dáº«n file .txt Ä‘á»ƒ lÆ°u.
        class_labels (dict): Mapping class_id â†’ tÃªn nhÃ£n (str).
        use_label_str (bool): Náº¿u True thÃ¬ ghi ra nhÃ£n dáº¡ng string; náº¿u False thÃ¬ ghi class_id sá»‘.
        conf_threshold (float): NgÆ°á»¡ng confidence tá»‘i thiá»ƒu.
    """
    with open(txt_filename, "w") as f:
        for box in result.boxes:
            if box.conf[0] > conf_threshold:
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                x1, y1, x2, y2 = map(float, box.xyxy[0])

                label = class_labels.get(cls, f"Unknown{cls}") if use_label_str else cls
                f.write(f"{label} {conf:.4f} {x1:.2f} {y1:.2f} {x2:.2f} {y2:.2f}\n")


def process_videos(input_folder: str, output_folder: str, model_path: str) -> None:
    """
    Process all video files in the input folder using YOLO object detection.

    Args:
        input_folder (str): Path to the folder containing input videos.
        output_folder (str): Path to the folder where output images and logs will be saved.
        model_path (str): Path to the YOLO model (e.g., yolov8s.pt).
    
    Save Directory:
        original_frames: Contain orginal image
        txt_labels: Contain label image
        yolo_drawn: Contain yolo drawn image 
    """
    input_path = Path(input_folder)
    output_path = Path(output_folder)
    yolo_path = output_path / "yolo_drawn"
    original_path = output_path / "original_frames"
    txt_path = output_path / "txt_labels"

    yolo_path.mkdir(parents=True, exist_ok=True)
    original_path.mkdir(parents=True, exist_ok=True)
    txt_path.mkdir(parents=True, exist_ok=True)

    yolo = YOLO(model_path)
    video_files = collect_video_paths(str(input_path))

    global_frame_index = 0

    # Danh sÃ¡ch nhÃ£n class
    class_labels = {
        0: "Person",            # 0: "NgÆ°á»i",           0: "äºº (RÃ©n)", 
        1: "SafetyShoes",       # 1: "GiÃ y báº£o há»™",     1: "å®‰å…¨éž‹ (Ä€nquÃ¡n xiÃ©)",  
        2: "ESDSlippers",       # 2: "DÃ©p tÄ©nh Ä‘iá»‡n",   2: "é˜²é™ç”µæ‹–éž‹ (FÃ¡ng jÃ¬ngdiÃ n tuÅxiÃ©)",
        3: "Head",              # 3: "Äáº§u",             3: "å¤´éƒ¨ (TÃ³ubÃ¹)",
        4: "BlueUniform",       # 4: "Ão xanh",         4: "è“è‰²åˆ¶æœ (LÃ¡nsÃ¨ zhÃ¬fÃº)",
        5: "WhiteUniform",      # 5: "Ão tráº¯ng",        5: "ç™½è‰²åˆ¶æœ (BÃ¡isÃ¨ zhÃ¬fÃº)",  
        6: "BlackUniform",      # 6: "Ão Ä‘en",          6: "é»‘è‰²åˆ¶æœ (HÄ“isÃ¨ zhÃ¬fÃº)", 
        7: "OtherUniform",      # 7: "Ão KhÃ¡c",         7: "å…¶ä»–åˆ¶æœ (QÃ­tÄ zhÃ¬fÃº)",
        8: "Bending",           # 8: "CÃºi",             8: "å¼¯è…° (WÄnyÄo)",
        9: "FireExtinguisher",  # 9: "BÃ¬nh Cá»©u Há»a",    9: "ç­ç«å™¨ (MiÃ¨huÇ’qÃ¬)",  
    }

    for video_file in video_files:
        cap = cv2.VideoCapture(str(video_file))

        while True:
            ret, frame = cap.read()
            if not ret: break

            results = yolo(frame)
            if not results: continue
            result = results[0]

            # Save original frame
            orig_filename = original_path / f"{global_frame_index:06d}.jpg"
            cv2.imwrite(str(orig_filename), frame)

            # Save annotated frame
            annotated_frame = result.plot()
            yolo_filename = yolo_path / f"{global_frame_index:06d}.jpg"
            cv2.imwrite(str(yolo_filename), annotated_frame)

            # Save detection data to txt with readable label
            txt_filename = txt_path / f"{global_frame_index:06d}.txt"
            write_yolo_txt_with_labels(result, txt_filename, class_labels)

            global_frame_index += 1
        cap.release()

    print(f"Processing completed. Results saved in {output_path}")

def inference_cam360_dataset() -> None:
    """Run a predefined command to evaluate all videos using the process_videos script."""
    process_videos(
        input_folder=f"{os.getcwd()}/data/Cam360",
        output_folder=f"{os.getcwd()}/evals/Train_Fulian_25_04_20252",
        model_path=f"{os.getcwd()}/data/Cam360/Weight/Train_Fulian_25_04_20252/weights/best.pt")

# -------------------------------------- Inference with cam360 datasets -------------------------------------- 
# -------------------------------------- Self-Label-Repair-System -------------------------------------- 

# TODO: Tomorrow run this 
def load_image(image_path: Path) -> torch.Tensor:
    """
    Load áº£nh tá»« Ä‘Æ°á»ng dáº«n vÃ  chuyá»ƒn vá» tensor PyTorch [3, H, W] kiá»ƒu uint8.

    Args:
        image_path (Path): ÄÆ°á»ng dáº«n tá»›i file áº£nh.

    Returns:
        torch.Tensor: áº¢nh dáº¡ng tensor RGB [3, H, W], dtype=uint8.
    """
    image = cv2.imread(str(image_path)) # BGR
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # RGB
    tensor = torch.from_numpy(image).permute(2, 0, 1) # [H, W, C] -> [C, H, W]
    return tensor


def read_yolo_labels(label_path: Path) -> List[str]:
    """
    Äá»c file nhÃ£n YOLO (.txt), tráº£ vá» list cÃ¡c dÃ²ng.

    Args:
        label_path (Path): ÄÆ°á»ng dáº«n tá»›i file label.

    Returns:
        List[str]: Danh sÃ¡ch cÃ¡c dÃ²ng nhÃ£n.
    """
    with open(label_path, 'r') as f:
        return [line.strip() for line in f.readlines()]


def crop_object(image: torch.Tensor, box: List[float]) -> Image.Image:
    """
    Crop object tá»« áº£nh dá»±a vÃ o tá»a Ä‘á»™ YOLO vÃ  tráº£ vá» áº£nh PIL.

    Args:
        image (torch.Tensor): Tensor áº£nh [3, H, W].
        box (List[float]): Tá»a Ä‘á»™ bounding box [x1, y1, x2, y2].

    Returns:
        Image.Image: áº¢nh Ä‘Ã£ crop (PIL).
    """
    x1, y1, x2, y2 = map(int, box)
    cropped = image[:, y1:y2, x1:x2]
    if cropped.shape[1] == 0 or cropped.shape[2] == 0:
        return None  # Box lá»—i
    pil = Image.fromarray(cropped.permute(1, 2, 0).cpu().numpy().astype("uint8"))
    return pil


def verify_object_label(
    object_image: Image.Image,
    label_name: str,
    tokenizer,
    model
) -> Tuple[bool, str]:
    """
    DÃ¹ng MMLLM xÃ¡c minh xem object trong áº£nh cÃ³ khá»›p vá»›i label_name khÃ´ng.

    Args:
        object_image (Image.Image): áº¢nh Ä‘Ã£ crop.
        label_name (str): TÃªn label dá»± Ä‘oÃ¡n tá»« YOLO.
        tokenizer, model: MiniCPM model vÃ  tokenizer.

    Returns:
        Tuple[bool, str]: (cÃ³ khá»›p khÃ´ng, tÃªn label mÃ´ hÃ¬nh nghÄ© lÃ  gÃ¬).
    """
    prompt = f"Is this a {label_name}?"
    msgs = [{'role': 'user', 'content': [object_image, prompt]}]
    answer: str = model.chat(image=None, msgs=msgs, tokenizer=tokenizer)
    return "yes" in answer.lower(), answer.strip()


def run_relabel_pipeline(
    data_dir: str = "./evals/Train_Fulian_25_04_20252",
    output_dir: str = "./relabel_data",
    limit: int = 1000,
    model_name: str = "openbmb/MiniCPM-o-2_6"
) -> None:
    """
    Pipeline xÃ¡c minh tá»«ng object vÃ  sá»­a label sai. LÆ°u káº¿t quáº£ Ä‘á»ƒ fine-tune YOLO.

    Args:
        data_dir (str): ThÆ° má»¥c chá»©a áº£nh vÃ  nhÃ£n gá»‘c.
        output_dir (str): ThÆ° má»¥c lÆ°u áº£nh/nhÃ£n Ä‘Ã£ sá»­a.
        limit (int): Sá»‘ lÆ°á»£ng tá»‘i Ä‘a máº«u sá»­a Ä‘á»ƒ thu tháº­p.
        model_name (str): TÃªn mÃ´ hÃ¬nh MiniCPM.
    """
    input_image_dir = Path(data_dir) / "original_frames"
    input_label_dir = Path(data_dir) / "txt_labels"
    output_image_dir = Path(output_dir) / "images"
    output_label_dir = Path(output_dir) / "labels"

    output_image_dir.mkdir(parents=True, exist_ok=True)
    output_label_dir.mkdir(parents=True, exist_ok=True)

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModel.from_pretrained(
        model_name,
        trust_remote_code=True,
        attn_implementation="sdpa",
        torch_dtype=torch.bfloat16,
        init_vision=True,
        init_audio=True,
        init_tts=True
    ).to("cuda").eval()

    class_labels = {
        0: "Person", 1: "SafetyShoes", 2: "ESDSlippers", 3: "Head",
        4: "BlueUniform", 5: "WhiteUniform", 6: "BlackUniform",
        7: "OtherUniform", 8: "Bending", 9: "FireExtinguisher"
    }
    label2id = {v: k for k, v in class_labels.items()}

    image_paths = sorted(input_image_dir.glob("*.jpg"))
    collected = 0

    for img_path in image_paths:
        if collected >= limit:
            break
        label_path = input_label_dir / f"{img_path.stem}.txt"
        if not label_path.exists():
            continue

        image_tensor = load_image(img_path)
        label_lines = read_yolo_labels(label_path)
        new_labels = []

        for line in label_lines:
            parts = line.strip().split()
            if len(parts) != 6:
                continue
            cls_id = int(parts[0])
            label_name = class_labels.get(cls_id, "Unknown")
            box = list(map(float, parts[2:6]))  # x1 y1 x2 y2
            obj_img = crop_object(image_tensor, box)
            if obj_img is None:
                continue

            match, predicted = verify_object_label(obj_img, label_name, tokenizer, model)
            if match:
                new_labels.append(line)
            else:
                pred_id = label2id.get(predicted, -1)
                if pred_id == -1:
                    continue
                new_line = f"{pred_id} {parts[1]} {' '.join(parts[2:])}"
                new_labels.append(new_line)

        # LÆ°u náº¿u cÃ³ Ã­t nháº¥t 1 label bá»‹ sá»­a
        if new_labels and new_labels != label_lines:
            shutil.copy(img_path, output_image_dir / img_path.name)
            with open(output_label_dir / label_path.name, "w") as f:
                for line in new_labels:
                    f.write(line + "\n")
            collected += 1

    print(f"ðŸ” Collected {collected} relabeled images at {output_dir}")


# -------------------------------------- Self-Label-Repair-System -------------------------------------- 
# -------------------------------------- Fine-tune mÃ´ hÃ¬nh YOLO má»›i tá»« cÃ¡c áº£nh Ä‘Ã£ xÃ¡c thá»±c -------------------------------------- 


def fine_tune_yolo_on_verified_data(
    verified_data_dir: str = "./verified_samples",
    old_model_path: str = "./data/Cam360/Weight/Train_Fulian_25_04_20252/weights/best.pt",
    save_dir: str = "./trained_new_yolo",
    epochs: int = 20,
    imgsz: int = 640
) -> Path:
    """
    Fine-tune láº¡i YOLO tá»« model cÅ© báº±ng táº­p dá»¯ liá»‡u Ä‘Ã£ xÃ¡c thá»±c.

    Args:
        verified_data_dir (str): ThÆ° má»¥c chá»©a "images" vÃ  "labels" tá»« bÆ°á»›c xÃ¡c thá»±c.
        old_model_path (str): Trá»ng sá»‘ mÃ´ hÃ¬nh YOLO cÅ©.
        save_dir (str): NÆ¡i lÆ°u káº¿t quáº£ training má»›i.
        epochs (int): Sá»‘ epoch Ä‘á»ƒ huáº¥n luyá»‡n.
        imgsz (int): KÃ­ch thÆ°á»›c áº£nh khi train.

    Returns:
        Path: ÄÆ°á»ng dáº«n Ä‘áº¿n mÃ´ hÃ¬nh tá»‘t nháº¥t sau fine-tune.
    """
    data_yaml_path = Path(verified_data_dir) / "data.yaml"

    # Táº¡o file data.yaml cho YOLO
    with open(data_yaml_path, "w") as f:
        f.write(f"""
path: {verified_data_dir}
train: images
val: images

names:
0: Person
1: SafetyShoes
2: ESDSlippers
3: Head
4: BlueUniform
5: WhiteUniform
6: BlackUniform
7: OtherUniform
8: Bending
9: FireExtinguisher
""")

    # Fine-tune YOLO
    model = YOLO(old_model_path)
    model.train(
        data=str(data_yaml_path),
        epochs=epochs,
        imgsz=imgsz,
        save=True,
        save_dir=save_dir,
        project=None,
        name=None
    )
    return Path(save_dir) / "weights" / "best.pt"


def evaluate_model(
    model_path: str,
    data_dir: str = "./evals/Train_Fulian_25_04_20252",
    imgsz: int = 640
) -> float:
    """
    ÄÃ¡nh giÃ¡ mAP cá»§a má»™t mÃ´ hÃ¬nh YOLO trÃªn táº­p dá»¯ liá»‡u Ä‘á»‹nh trÆ°á»›c.

    Args:
        model_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n model YOLO Ä‘á»‹nh dáº¡ng .pt
        data_dir (str): ThÆ° má»¥c chá»©a áº£nh vÃ  nhÃ£n (original_frames + txt_labels)
        imgsz (int): KÃ­ch thÆ°á»›c áº£nh resize khi Ä‘Ã¡nh giÃ¡

    Returns:
        float: GiÃ¡ trá»‹ mAP50 thu Ä‘Æ°á»£c
    """
    # Táº¡o file YAML mÃ´ táº£ dá»¯ liá»‡u táº¡m thá»i
    yaml_path = Path(data_dir) / "eval_data.yaml"
    with open(yaml_path, "w") as f:
        f.write(f"""
path: {data_dir}
train: original_frames
val: original_frames
names:
0: Person
1: SafetyShoes
2: ESDSlippers
3: Head
4: BlueUniform
5: WhiteUniform
6: BlackUniform
7: OtherUniform
8: Bending
9: FireExtinguisher
""")

    # Load model vÃ  thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡
    model = YOLO(model_path)
    metrics = model.val(data=str(yaml_path), imgsz=imgsz, split="val", save=False)
    return metrics.box.map50


def replace_model_if_better(
    old_model_path: str,
    new_model_path: str,
    eval_data_dir: str = "./evals/Train_Fulian_25_04_20252"
) -> None:
    """
    So sÃ¡nh model má»›i vÃ  model cÅ©, náº¿u model má»›i tá»‘t hÆ¡n (mAP cao hÆ¡n) thÃ¬ thay tháº¿.

    Args:
        old_model_path (str): Trá»ng sá»‘ mÃ´ hÃ¬nh Ä‘ang dÃ¹ng
        new_model_path (str): Trá»ng sá»‘ mÃ´ hÃ¬nh má»›i huáº¥n luyá»‡n
        eval_data_dir (str): ThÆ° má»¥c dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ (áº£nh + label)
    """
    print("\nðŸ” Evaluating old model...")
    old_map = evaluate_model(old_model_path, data_dir=eval_data_dir)

    print("\nðŸ” Evaluating new model...")
    new_map = evaluate_model(new_model_path, data_dir=eval_data_dir)

    print(f"\nðŸ“Š mAP50 - old: {old_map:.4f} | new: {new_map:.4f}")

    if new_map > old_map:
        print("âœ… New model is better. Replacing old model...")
        os.replace(new_model_path, old_model_path)
    else:
        print("â›” Old model is better. Keeping original model.")


def run_self_label_repair_system():
    """
    Cháº¡y toÃ n bá»™ pipeline self-label-repair:
    1. XÃ¡c thá»±c láº¡i nhÃ£n báº±ng MMLLM (MiniCPM)
    2. Fine-tune YOLO trÃªn áº£nh Ä‘Ã£ xÃ¡c thá»±c
    3. So sÃ¡nh hiá»‡u quáº£ vÃ  thay tháº¿ model náº¿u cáº§n
    """
    from self_label_repair import run_verification_pipeline, fine_tune_yolo_on_verified_data

    run_verification_pipeline()
    best_new_model = fine_tune_yolo_on_verified_data()

    replace_model_if_better(
        old_model_path="./data/Cam360/Weight/Train_Fulian_25_04_20252/weights/best.pt",
        new_model_path=str(best_new_model)
    )

# -------------------------------------- Fine-tune mÃ´ hÃ¬nh YOLO má»›i tá»« cÃ¡c áº£nh Ä‘Ã£ xÃ¡c thá»±c -------------------------------------- 


if __name__ == "__main__":
    fire.Fire({
        "run_inference_cam360_dataset": inference_cam360_dataset, 
        "relabel_objects": run_relabel_pipeline,
        "run_self_label_repair": run_self_label_repair_system, 
    })
