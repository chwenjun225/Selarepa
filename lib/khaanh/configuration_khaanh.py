import os 
from typing import Union 
from typing import Optional
from typing import Any 
from typing import Dict 

from transformers import PretrainedConfig 
from transformers import Qwen2Config
from transformers import WhisperConfig
from transformers.utils import logging

from .modeling_navit_siglip import SiglipVisionConfig

logger = logging.get_logger(__name__)

class KhaanhSliceConfig(PretrainedConfig):
    """
    C·∫•u h√¨nh cho module x·ª≠ l√Ω slice ·∫£nh trong m√¥ h√¨nh.
    Bao g·ªìm c√°c tham s·ªë k√≠ch th∆∞·ªõc patch, s·ªë l√°t ·∫£nh t·ªëi ƒëa, 
    v√† ƒë·ªô ph√¢n gi·∫£i chu·∫©n d√πng ƒë·ªÉ resize ·∫£nh.

    Attributes:
        patch_size (int): K√≠ch th∆∞·ªõc m·ªói patch (v√≠ d·ª•: 14 x 14 pixel).
        max_slice_nums (int): S·ªë l∆∞·ª£ng l√°t ·∫£nh t·ªëi ƒëa c√≥ th·ªÉ chia.
        scale_resolution (int): ƒê·ªô ph√¢n gi·∫£i chu·∫©n ƒë·ªÉ scale ·∫£nh v·ªÅ tr∆∞·ªõc khi chia l√°t.
    """
    model_type = "khaanh"

    def __init__(
            self,
            patch_size: int = 14,
            max_slice_nums: int = 9,
            scale_resolution: int = 448,
            **kwargs,
        ):
        """
        Kh·ªüi t·∫°o c·∫•u h√¨nh slice cho m√¥ h√¨nh MiniCPM-o.

        Args:
            patch_size (int, optional): K√≠ch th∆∞·ªõc m·ªói patch. M·∫∑c ƒë·ªãnh l√† 14.
            max_slice_nums (int, optional): S·ªë l√°t ·∫£nh t·ªëi ƒëa. M·∫∑c ƒë·ªãnh l√† 9.
            scale_resolution (int, optional): ƒê·ªô ph√¢n gi·∫£i chu·∫©n. M·∫∑c ƒë·ªãnh l√† 448.
            **kwargs: C√°c tham s·ªë c·∫•u h√¨nh b·ªï sung kh√°c.
        """
        super().__init__(**kwargs)
        self.patch_size = patch_size
        self.max_slice_nums = max_slice_nums
        self.scale_resolution = scale_resolution

    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Union[str, os.PathLike],
        **kwargs
    ) -> "PretrainedConfig":
        """
        T·∫£i c·∫•u h√¨nh t·ª´ t√™n m√¥ h√¨nh ho·∫∑c ƒë∆∞·ªùng d·∫´n ƒë√£ hu·∫•n luy·ªán tr∆∞·ªõc.

        Args:
            pretrained_model_name_or_path (Union[str, os.PathLike]): T√™n m√¥ h√¨nh tr√™n Hugging Face Hub
                ho·∫∑c ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a t·ªáp c·∫•u h√¨nh.
            **kwargs: C√°c tham s·ªë b·ªï sung (v√≠ d·ª•: cache_dir, force_download, revision, v.v.).

        Returns:
            PretrainedConfig: M·ªôt ƒë·ªëi t∆∞·ª£ng c·∫•u h√¨nh ƒë∆∞·ª£c kh·ªüi t·∫°o t·ª´ file c·∫•u h√¨nh pretrained.
        """
        # G·∫Øn token n·∫øu c·∫ßu (d√†nh cho vi·ªác t·∫£i m√¥ h√¨nh private t·ª´ ü§ó Hub)
        cls._set_token_in_kwargs(kwargs)

        # ƒê·ªçc config t·ª´ file JSON ho·∫∑c remote model hub 
        config_dict, kwargs = cls.get_config_dict(
            pretrained_model_name_or_path, **kwargs
        )
        # L·∫•y ph·∫ßn c·∫•u h√¨nh cho "slice_config":
        if config_dict.get("model_type") == "minicpmv":
            config_dict = config_dict["slice_config"]

        # C·∫£nh b√°o n·∫øu model_type trong file config kh√¥ng tr√πng v·ªõi class n√†y
        if "model_type" in config_dict and hasattr(cls, "model_type") and config_dict["model_type"] != cls.model_type:
            logger.warning(
                f"You are using a model of type {config_dict['model_type']} to instantiate a model of type "
                f"{cls.model_type}. This is not supported for all configurations of models and can yield errors."
            )
        return cls.from_dict(config_dict, **kwargs)

class ConditionalChatTTSConfig(PretrainedConfig):
    """
    C·∫•u h√¨nh cho m√¥ h√¨nh Conditional-Chat-Text-To-Speech.

    Attributes:
        - Ki·∫øn tr√∫c m√¥ h√¨nh (transformer dimensions, attention, hidden layers, etc)
        - S·ªë l∆∞·ª£ng token (√¢m thanh, vƒÉn b·∫£n)
        - C·∫•u h√¨nh streaming/inference
        - C√°c t√πy ch·ªçn cho sampling v√† loss
    """
    model_type = "conditional_chattts"

    def __init__(
        self,
        llm_dim: int = 2560,
        hidden_size: int = 768,
        intermediate_size: int = 3072,
        num_attention_heads: int = 12,
        num_hidden_layers: int = 20,
        max_position_embeddings: int = 4096,
        num_audio_tokens: int = 626,
        num_text_tokens: int = 21178,
        num_mel_bins: int = 100,
        num_vq: int = 4,
        use_speaker_embedding: bool = True,
        use_llm_hidden_state: bool = False,
        spk_emb_token_id: int = 21143,
        num_spk_embs: int = 1,
        audio_bos_token_id: int = 21132,
        text_eos_token_id: int = 21133,
        use_text: bool = True,
        streaming: bool = True,
        streaming_text_chunk_size: int = 10,
        streaming_text_reserved_len: int = 300,
        streaming_audio_chunk_size: int = 50,
        attn_implementation: str = "sdpa",
        use_mlp: bool = True,
        aug_loss_weight: bool = True,
        do_sample: bool = True,
        top_p: float = 0.7,
        top_k: int = 20,
        repetition_penalty: float = 1.0,
        **kwargs,
    ):
        """
        Kh·ªüi t·∫°o c·∫•u h√¨nh cho Conditional ChatTTS.

        Args:
            llm_dim (int): K√≠ch th∆∞·ªõc ·∫©n c·ªßa ƒë·∫ßu ra t·ª´ LLM (n·∫øu d√πng).
            hidden_size (int): K√≠ch th∆∞·ªõc ·∫©n c·ªßa transformer.
            intermediate_size (int): K√≠ch th∆∞·ªõc t·∫ßng FFN.
            num_attention_heads (int): S·ªë l∆∞·ª£ng ƒë·∫ßu attention.
            num_hidden_layers (int): S·ªë t·∫ßng transformer.
            max_position_embeddings (int): Chi·ªÅu d√†i chu·ªói ƒë·∫ßu v√†o t·ªëi ƒëa.
            num_audio_tokens (int): S·ªë l∆∞·ª£ng token trong kh√¥ng gian m√£ h√≥a √¢m thanh.
            num_text_tokens (int): S·ªë l∆∞·ª£ng token t·ª´ tokenizer vƒÉn b·∫£n.
            num_mel_bins (int): S·ªë l∆∞·ª£ng bins trong spectrogram ƒë·∫ßu ra.
            num_vq (int): S·ªë l∆∞·ª£ng vector quantizers (VQ) d√πng ƒë·ªÉ m√£ h√≥a √¢m thanh.
            use_speaker_embedding (bool): C√≥ s·ª≠ d·ª•ng embedding ng∆∞·ªùi n√≥i kh√¥ng.
            use_llm_hidden_state (bool): C√≥ d√πng hidden state t·ª´ LLM ƒë·ªÉ ƒëi·ªÅu ki·ªán h√≥a kh√¥ng.
            spk_emb_token_id (int): ID token ƒë·∫°i di·ªán cho embedding ng∆∞·ªùi n√≥i.
            num_spk_embs (int): S·ªë l∆∞·ª£ng embedding ng∆∞·ªùi n√≥i c√≥ th·ªÉ d√πng.
            audio_bos_token_id (int): ID cho token b·∫Øt ƒë·∫ßu chu·ªói √¢m thanh.
            text_eos_token_id (int): ID cho token k·∫øt th√∫c vƒÉn b·∫£n.
            use_text (bool): C√≥ s·ª≠ d·ª•ng ƒë·∫ßu v√†o vƒÉn b·∫£n kh√¥ng (c√≥ th·ªÉ False n·∫øu ch·ªâ d·ª± ƒëo√°n ti·∫øp √¢m thanh).
            streaming (bool): C√≥ b·∫≠t ch·∫ø ƒë·ªô streaming kh√¥ng.
            streaming_text_chunk_size (int): K√≠ch th∆∞·ªõc kh·ªëi vƒÉn b·∫£n m·ªói l·∫ßn streaming.
            streaming_text_reserved_len (int): ƒê·ªô d√†i b·ªô nh·ªõ gi·ªØ l·∫°i gi·ªØa c√°c chunk vƒÉn b·∫£n khi streaming.
            streaming_audio_chunk_size (int): K√≠ch th∆∞·ªõc kh·ªëi √¢m thanh khi streaming.
            attn_implementation (str): Ki·ªÉu attention s·ª≠ d·ª•ng (vd: "sdpa", "flash").
            use_mlp (bool): C√≥ d√πng MLP trong ki·∫øn tr√∫c kh√¥ng.
            aug_loss_weight (bool): C√≥ d√πng tr·ªçng s·ªë cho loss b·ªï sung kh√¥ng.
            do_sample (bool): C√≥ d√πng sampling trong qu√° tr√¨nh generate kh√¥ng.
            top_p (float): Tham s·ªë nucleus sampling (p).
            top_k (int): Tham s·ªë top-k sampling.
            repetition_penalty (float): Ph·∫°t l·∫∑p l·∫°i token khi generate.
            **kwargs: C√°c tham s·ªë c·∫•u h√¨nh b·ªï sung kh√°c.
        """
        super().__init__(**kwargs)
        
        # C·∫•u h√¨nh ki·∫øn tr√∫c
        self.llm_dim = llm_dim
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.max_position_embeddings = max_position_embeddings

        # Token & embedding
        self.num_audio_tokens = num_audio_tokens
        self.num_text_tokens = num_text_tokens
        self.num_mel_bins = num_mel_bins
        self.num_vq = num_vq

        # C·∫•u h√¨nh gi·ªçng n√≥i v√† LLM
        self.use_speaker_embedding = use_speaker_embedding
        self.use_llm_hidden_state = use_llm_hidden_state
        self.spk_emb_token_id = spk_emb_token_id
        self.num_spk_embs = num_spk_embs
        self.audio_bos_token_id = audio_bos_token_id
        self.text_eos_token_id = text_eos_token_id

        # T√πy ch·ªçn x·ª≠ l√Ω ƒë·∫ßu v√†o
        self.use_text = use_text

        # Streaming inference
        self.streaming = streaming
        self.streaming_text_chunk_size = streaming_text_chunk_size
        self.streaming_text_reserved_len = streaming_text_reserved_len
        self.streaming_audio_chunk_size = streaming_audio_chunk_size

        # C√°c k·ªπ thu·∫≠t c·∫£i ti·∫øn
        self.attn_implementation = attn_implementation
        self.use_mlp = use_mlp
        self.aug_loss_weight = aug_loss_weight

        # T√πy ch·ªçn sampling
        self.do_sample = do_sample
        self.top_p = top_p
        self.top_k = top_k
        self.repetition_penalty = repetition_penalty

class KhaanhConfig(Qwen2Config):
    """
    C·∫•u h√¨nh t·ªïng h·ª£p cho m√¥ h√¨nh Khaanh (Khaanh-Multi-Modal),
    t√≠ch h·ª£p c√°c m√¥ ƒëun x·ª≠ l√Ω vƒÉn b·∫£n, h√¨nh ·∫£nh, √¢m thanh, v√† text-to-speech (TTS).

    K·∫ø th·ª´a t·ª´ Qwen2Config v√† m·ªü r·ªông v·ªõi c√°c c·∫•u h√¨nh ph·ª•:
    - Vision (SigLIP)
    - Audio (Whisper)
    - TTS (ConditionalChatTTS)
    - Slice (KhaanhSliceConfig)
    """
    model_type = "khaanh"
    keys_to_ignore_at_inference = ["past_key_values"]

    # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho m√¥ h√¨nh vision SigLIP n·∫øu kh√¥ng cung c·∫•p
    default_vision_config = {
        "hidden_size": 1152,
        "image_size": 980,
        "intermediate_size": 4304,
        "model_type": "siglip",
        "num_attention_heads": 16,
        "num_hidden_layers": 27,
        "patch_size": 14,
    }

    def __init__(
            self,
            use_cache: bool = True,
            query_num: int = 64,
            image_size: int = 448,
            drop_vision_last_layer: bool = True,
            batch_vision_input: bool = True,
            slice_config: Optional[Union[Dict[str, Any], KhaanhSliceConfig]] = None,
            vision_config: Optional[Union[Dict[str, Any], SiglipVisionConfig]] = None,
            audio_config: Optional[Union[Dict[str, Any], WhisperConfig]] = None,
            tts_config: Optional[Union[Dict[str, Any], ConditionalChatTTSConfig]] = None,
            use_image_id: bool = True,
            vision_batch_size: int = 16,
            audio_pool_step: int = 2,
            audio_chunk_length: float = 1.0,
            stream_input: bool = False,
            init_vision: bool = True,
            init_audio: bool = True,
            init_tts: bool = True,
            **kwargs,
        ):
        """
        Khai b√°o c·∫•u tr√∫c v√† h√†nh vi cho c√°c m√¥ ƒëun ph·ª• c·ªßa m√¥ h√¨nh.

        Args:
            use_cache (bool): C√≥ s·ª≠ d·ª•ng b·ªô nh·ªõ attention trong inference kh√¥ng.
            query_num (int): S·ªë l∆∞·ª£ng query token d√πng ƒë·ªÉ n·∫°p th√¥ng tin ·∫£nh v√†o LLM.
            image_size (int): K√≠ch th∆∞·ªõc ·∫£nh (vu√¥ng).
            drop_vision_last_layer (bool): C√≥ lo·∫°i b·ªè l·ªõp cu·ªëi c·ªßa encoder ·∫£nh kh√¥ng.
            batch_vision_input (bool): C√≥ x·ª≠ l√Ω batch ·∫£nh hay t·ª´ng ·∫£nh l·∫ª.
            slice_config (Union[dict, MiniCPMVSliceConfig], optional): C·∫•u h√¨nh chia l√°t ·∫£nh.
            vision_config (Union[dict, SiglipVisionConfig], optional): C·∫•u h√¨nh encoder ·∫£nh (SigLIP).
            audio_config (Union[dict, WhisperConfig], optional): C·∫•u h√¨nh cho m√¥ h√¨nh √¢m thanh (Whisper).
            tts_config (Union[dict, ConditionalChatTTSConfig], optional): C·∫•u h√¨nh cho m√¥ h√¨nh TTS.
            use_image_id (bool): C√≥ s·ª≠ d·ª•ng ID ·∫£nh trong placeholder kh√¥ng.
            vision_batch_size (int): Batch size cho x·ª≠ l√Ω ·∫£nh.
            audio_pool_step (int): S·ªë b∆∞·ªõc stride trong pooling encoder √¢m thanh.
            audio_chunk_length (float): ƒê·ªô d√†i m·ªói ƒëo·∫°n √¢m thanh (t√≠nh b·∫±ng gi√¢y).
            stream_input (bool): C√≥ b·∫≠t ch·∫ø ƒë·ªô streaming input kh√¥ng.
            init_vision (bool): Kh·ªüi t·∫°o encoder ·∫£nh khi load kh√¥ng.
            init_audio (bool): Kh·ªüi t·∫°o encoder √¢m thanh khi load kh√¥ng.
            init_tts (bool): Kh·ªüi t·∫°o TTS decoder khi load kh√¥ng.
            **kwargs: C√°c tham s·ªë kh√°c k·∫ø th·ª´a t·ª´ Qwen2Config.
        """
        # C√°c tham s·ªë t·ªïng qu√°t
        self.use_cache = use_cache
        self.query_num = query_num
        self.image_size = image_size
        self.drop_vision_last_layer = drop_vision_last_layer
        self.batch_vision_input = batch_vision_input
        self.use_image_id = use_image_id
        self.vision_batch_size = vision_batch_size
        self.audio_pool_step = audio_pool_step
        self.audio_chunk_length = audio_chunk_length
        self.stream_input = stream_input
        self.init_vision = init_vision
        self.init_audio = init_audio
        self.init_tts = init_tts

        # Slice config
        if slice_config is None:
            self.slice_config = KhaanhSliceConfig(max_slice_nums=1)
        else:
            self.slice_config = (
                slice_config if isinstance(slice_config, KhaanhSliceConfig)
                else KhaanhSliceConfig(**slice_config)
            )
        self.slice_mode = True  # Lu√¥n b·∫≠t chia l√°t ·∫£nh n·∫øu d√πng Khaanh

        # Vision config (d√πng SigLIP)
        if vision_config is None:
            self.vision_config = SiglipVisionConfig(**self.default_vision_config)
            logger.info("vision_config is None, using default vision config")
        elif isinstance(vision_config, dict):
            self.vision_config = SiglipVisionConfig(**vision_config)
        else:
            self.vision_config = vision_config

        # Audio config (d√πng Whisper)
        if audio_config is None:
            self.audio_config = WhisperConfig()
        elif isinstance(audio_config, dict):
            self.audio_config = WhisperConfig(**audio_config)
        else:
            self.audio_config = audio_config

        # TTS config
        if tts_config is None:
            self.tts_config = ConditionalChatTTSConfig()
        elif isinstance(tts_config, dict):
            self.tts_config = ConditionalChatTTSConfig(**tts_config)
        else:
            self.tts_config = tts_config

        # L∆∞u patch_size ƒë·ªÉ c√°c module con d·ªÖ truy c·∫≠p
        self.patch_size = self.vision_config.patch_size

        # G·ªçi constructor cha (Qwen2)
        super().__init__(**kwargs)
